{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "My_CXpFlxUvA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing"
      ],
      "metadata": {
        "id": "7vpa977YX8w4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "e4mUB-lFrI5U"
      },
      "outputs": [],
      "source": [
        "def load_dataset_timestamp(n_users, n_context, seq_len):\n",
        "    act_list = []\n",
        "    time_list = []\n",
        "    user_list = []\n",
        "\n",
        "    max_timestamp = -1.0\n",
        "    min_timestamp = float('inf')\n",
        "\n",
        "    with open('gowalla_user_activity.txt', 'r') as raw_file:\n",
        "        for line in raw_file:\n",
        "            t_item_list = []\n",
        "            t_time_list = []\n",
        "            user = int(line.split(':')[0])\n",
        "            entries = line.split()[1:]\n",
        "            for a_entry in entries:\n",
        "                item, time_stamp = a_entry.split(':')\n",
        "                t_item_list.append(int(item.strip()))\n",
        "                t_time_list.append(int(time_stamp.strip()))\n",
        "\n",
        "                if min_timestamp > int(time_stamp.strip()):\n",
        "                    min_timestamp = int(time_stamp.strip())\n",
        "                if max_timestamp < int(time_stamp.strip()):\n",
        "                    max_timestamp = int(time_stamp.strip())\n",
        "\n",
        "            act_list.append(t_item_list[0: seq_len])\n",
        "            time_list.append(t_time_list[0: seq_len])\n",
        "            user_list.append(user)\n",
        "\n",
        "    new_time_list = []\n",
        "    num_bins = 0\n",
        "\n",
        "    times_bins = np.linspace(min_timestamp, max_timestamp + 1, num=num_bins, dtype=np.int32)\n",
        "    for a_time_list in time_list:\n",
        "        temp_time_list = (np.digitize(np.asarray(a_time_list), times_bins) - 1).tolist()\n",
        "        new_time_list.append(temp_time_list)\n",
        "\n",
        "    all_examples = []\n",
        "    for i in range(0, len(act_list)):\n",
        "        train_act_seq = act_list[i][:-2]\n",
        "        train_time_seq = new_time_list[i][:-2]\n",
        "\n",
        "        train_act_label = act_list[i][-2]\n",
        "        train_time_label = new_time_list[i][-2]\n",
        "\n",
        "        test_act_seq = act_list[i][1:-1]\n",
        "        test_time_seq = new_time_list[i][1:-1]\n",
        "\n",
        "        test_act_label = act_list[i][-1]\n",
        "        test_time_label = new_time_list[i][-1]\n",
        "\n",
        "        entry = {\n",
        "            'train_act_seq': train_act_seq,\n",
        "            'train_time_seq': train_time_seq,\n",
        "            'train_act_label': train_act_label,\n",
        "            'train_time_label': train_time_label,\n",
        "            'test_act_seq': test_act_seq,\n",
        "            'test_time_seq': test_time_seq,\n",
        "            'test_act_label': test_act_label,\n",
        "            'test_time_label': test_time_label,\n",
        "            'seq_len': len(train_act_seq),\n",
        "            'user': user_list[i]\n",
        "        }\n",
        "\n",
        "        all_examples.append(entry)\n",
        "\n",
        "    return all_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "v0rqrY1DutA4"
      },
      "outputs": [],
      "source": [
        "data_examples = load_dataset_timestamp(20001, 128, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset class"
      ],
      "metadata": {
        "id": "PVOJA_x9YBsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UserDataset():\n",
        "  def __init__(self, data, max_len):\n",
        "    self.data = data\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    user = self.data[idx]\n",
        "    seq_len = user['seq_len']\n",
        "\n",
        "    tr_act_seq = np.zeros((self.max_len,)).astype('int32')\n",
        "    tr_act_seq[:seq_len] = np.array(user['train_act_seq'])\n",
        "    tr_act_seq = np.transpose(tr_act_seq)\n",
        "\n",
        "    tr_time_seq = np.zeros((self.max_len,)).astype('int32')\n",
        "    tr_time_seq[:seq_len] = user['train_time_seq']\n",
        "    tr_time_seq = np.transpose(tr_time_seq)\n",
        "\n",
        "    t_act_seq = np.zeros((self.max_len, )).astype('int32')\n",
        "    t_act_seq[:seq_len] = user['test_act_seq']\n",
        "    t_act_seq = np.transpose(t_act_seq)\n",
        "\n",
        "    t_time_seq = np.zeros((self.max_len, )).astype('int32')\n",
        "    t_time_seq[:seq_len] = user['test_time_seq']\n",
        "    t_time_seq = np.transpose(t_time_seq)\n",
        "\n",
        "\n",
        "    return user['user'], tr_act_seq, \\\n",
        "    tr_time_seq, user['train_act_label'], \\\n",
        "    user['train_time_label'], t_act_seq, \\\n",
        "    t_time_seq, user['test_act_label'], \\\n",
        "    user['test_time_label'], user['seq_len']"
      ],
      "metadata": {
        "id": "QTAXo3sfo6eq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline model"
      ],
      "metadata": {
        "id": "-DQ3UGLOYE2X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yQM14ooSx9zr"
      },
      "outputs": [],
      "source": [
        "class RecModel(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(RecModel, self).__init__()\n",
        "    self.fc1 = nn.Linear(128, num_classes)\n",
        "    self.rnn = nn.RNN(128, 128, batch_first = True)\n",
        "    self.norm = nn.BatchNorm1d(128)\n",
        "\n",
        "  def forward(self, x, seq_len):\n",
        "\n",
        "    x, h = self.rnn(x)\n",
        "    hx = torch.zeros(x.shape[0], x.shape[2])\n",
        "    for i in range(hx.shape[0]):\n",
        "      hx[i] = x[i][seq_len[i] - 1]\n",
        "    hx = self.norm(hx)\n",
        "\n",
        "    x = self.fc1(hx)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset initialization"
      ],
      "metadata": {
        "id": "Erq4-cVqYJkL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "m0jLP4j-2rJJ"
      },
      "outputs": [],
      "source": [
        "num_classes = 186\n",
        "item_emb  = nn.init.xavier_uniform_(torch.empty(num_classes, 128))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "vjYZVy_mKiBh"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "user_dataset = UserDataset(data_examples, 100)\n",
        "\n",
        "n = len(user_dataset)\n",
        "\n",
        "indices = np.arange(n)\n",
        "indices = np.random.permutation(indices)\n",
        "\n",
        "train_indices = indices [:int(0.8*n)]\n",
        "test_indices = indices[int(0.8*n):]\n",
        "\n",
        "user_train_dataset = Subset(user_dataset, train_indices)\n",
        "user_test_dataset = Subset(user_dataset, test_indices)\n",
        "\n",
        "user_train_dataloader = DataLoader(user_train_dataset, batch_size=64, shuffle=True)\n",
        "user_test_dataloader = DataLoader(user_test_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jGDyi1F1OxDi"
      },
      "outputs": [],
      "source": [
        "labels = np.arange(0, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics"
      ],
      "metadata": {
        "id": "r1TTqI31YZNE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wveiHrarPAc2"
      },
      "outputs": [],
      "source": [
        "def apk(actual, predicted, k=10):\n",
        "    if len(predicted) > k:\n",
        "        predicted = predicted[:k]\n",
        "\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "\n",
        "    for i, p in enumerate(predicted):\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i + 1.0)\n",
        "\n",
        "    if not actual:\n",
        "        return 0.0\n",
        "\n",
        "    return score / min(len(actual), k)\n",
        "\n",
        "\n",
        "def mapk(y_prob, y, k=10):\n",
        "    predicted = [np.argsort(p_)[-k:][::-1] for p_ in y_prob]\n",
        "    actual = [[y_] for y_ in y]\n",
        "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
        "\n",
        "\n",
        "\n",
        "def hits_k(y_prob, y, k=10):\n",
        "    acc = []\n",
        "    for p_, y_ in zip(y_prob, y):\n",
        "        top_k = p_.argsort()[-k:][::-1]\n",
        "        acc += [1. if y_ in top_k else 0.]\n",
        "    return sum(acc) / len(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ZAIyJzxrNx7B"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import top_k_accuracy_score\n",
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "def get_metrics_(probs, labels_batch, test_one_hot):\n",
        "    hits1 = top_k_accuracy_score(labels_batch, probs.cpu().detach().numpy(), k=1, labels = labels)\n",
        "    hits5 = top_k_accuracy_score(labels_batch, probs.cpu().detach().numpy(), k=5, labels = labels)\n",
        "    hits10 = top_k_accuracy_score(labels_batch, probs.cpu().detach().numpy(), k=10, labels = labels)\n",
        "    hits20 = top_k_accuracy_score(labels_batch, probs.cpu().detach().numpy(), k=20, labels = labels)\n",
        "    hits50= top_k_accuracy_score(labels_batch, probs.cpu().detach().numpy(), k=50, labels = labels)\n",
        "    hits100 = top_k_accuracy_score(labels_batch, probs.cpu().detach().numpy(), k=100, labels = labels)\n",
        "\n",
        "    map1 = mapk(y_prob=probs.cpu().detach().numpy(), y = labels_batch, k=1)\n",
        "    map5 = mapk(y_prob=probs.cpu().detach().numpy(), y = labels_batch, k=5)\n",
        "    map10 = mapk(y_prob=probs.cpu().detach().numpy(), y = labels_batch, k=10)\n",
        "    map20 = mapk(y_prob=probs.cpu().detach().numpy(), y = labels_batch, k=20)\n",
        "    map50 = mapk(y_prob=probs.cpu().detach().numpy(), y = labels_batch, k=50)\n",
        "    map100 = mapk(y_prob=probs.cpu().detach().numpy(), y = labels_batch, k=100)\n",
        "\n",
        "    ndcg1 = ndcg_score(test_one_hot, probs.cpu().detach().numpy(), k=1)\n",
        "    ndcg5 = ndcg_score(test_one_hot, probs.cpu().detach().numpy(), k=5)\n",
        "    ndcg10 = ndcg_score(test_one_hot, probs.cpu().detach().numpy(), k=10)\n",
        "    ndcg20 = ndcg_score(test_one_hot, probs.cpu().detach().numpy(), k=20)\n",
        "    ndcg50 = ndcg_score(test_one_hot, probs.cpu().detach().numpy(), k=50)\n",
        "    ndcg100 = ndcg_score(test_one_hot, probs.cpu().detach().numpy(), k=100)\n",
        "    return hits1, hits5, hits10, hits20, hits50, hits100, map1, map5, map10, map20, map50, map100, ndcg1, ndcg5, ndcg10, ndcg20, ndcg50, ndcg100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FmEaZ3v-PPB2"
      },
      "outputs": [],
      "source": [
        "def print_metrics(hits1, hits5, hits10, hits20, hits50, hits100, map1, map5, map10, map20, map50, map100, ndcg1, ndcg5, ndcg10, ndcg20, ndcg50, ndcg100):\n",
        "    print(f'hits@1: {hits1:.6f}, hits@5: {hits5:.6f}, hits@10: {hits10:.6f}, hits@20: {hits20:.6f}')\n",
        "    print(f'hits@50: {hits50:.6f}, hits@100: {hits100:.6f}')\n",
        "    print(f'map@1: {map1:.6f}, map@5: {map5:.6f}, map@10: {map10:.6f}, map@20: {map20:.6f}')\n",
        "    print(f'map@50: {map50:.6f}, map@100: {map100:.6f}')\n",
        "    print(f'ndcg@1: {ndcg1:.6f}, ndcg@5: {ndcg5:.6f}, ndcg@10: {ndcg10:.6f}, ndcg@20: {ndcg20:.6f}')\n",
        "    print(f'ndcg@50: {ndcg50:.6f}, ndcg@100: {ndcg100:.6f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and test"
      ],
      "metadata": {
        "id": "rvIvGE-2Ybzw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "W_-IzqM4QzgG"
      },
      "outputs": [],
      "source": [
        "def test(model):\n",
        "  metrics_val = []\n",
        "  model.eval()\n",
        "  index = 0\n",
        "\n",
        "  for user, train_input, train_time, train_label, train_time_label, test_input, test_time, test_label, test_time_label, seq_len in user_test_dataloader:\n",
        "\n",
        "      test_comb_input = np.concatenate([np.expand_dims(test_input, axis=-1),\n",
        "                                                np.expand_dims(test_time, axis=-1)], axis=2)\n",
        "      model_input = test_comb_input\n",
        "      model_output = test_label\n",
        "      test_rnn_input_emb = item_emb[model_input[:, :, 0]]\n",
        "      test_probs = model(test_rnn_input_emb, seq_len)\n",
        "\n",
        "      test_pred = torch.argmax(test_probs, axis = 1)\n",
        "\n",
        "      test_one_hot = torch.zeros(len(test_probs), num_classes)\n",
        "      test_one_hot[torch.arange(len(test_one_hot)), test_label] = 1\n",
        "      loss = loss_fn(test_probs, test_one_hot)\n",
        "\n",
        "      hits1, hits5, hits10, hits20, hits50, hits100, map1, map5, map10, map20, map50, map100, \\\n",
        "      ndcg1, ndcg5, ndcg10, ndcg20, ndcg50, ndcg100 = get_metrics_(test_probs, test_label, test_one_hot)\n",
        "\n",
        "      metrics_val.append([hits1, hits5, hits10, hits20, hits50, hits100, map1, map5, map10, map20, map50, map100, ndcg1, ndcg5, ndcg10, ndcg20, ndcg50, ndcg100])\n",
        "\n",
        "\n",
        "  mean = torch.Tensor(metrics_val).mean(axis=0)\n",
        "  test_hits1, test_hits5, test_hits10, test_hits20, test_hits50, test_hits100, \\\n",
        "  test_map1, test_map5, test_map10, test_map20, test_map50, test_map100, test_ndcg1, test_ndcg5, test_ndcg10, test_ndcg20, test_ndcg50, test_ndcg100 = mean\n",
        "  return test_hits1, test_hits5, test_hits10, test_hits20, test_hits50, test_hits100, test_map1, test_map5, test_map10, test_map20, test_map50, test_map100,\\\n",
        "  test_ndcg1, test_ndcg5, test_ndcg10, test_ndcg20, test_ndcg50, test_ndcg100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xiXSj_YHwgN",
        "outputId": "5a4b5908-2626-4306-9b46-4c79c730ef16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 1.5317010879516602\n",
            "Epoch: 1 Loss: 0.9467215538024902\n",
            "Epoch: 2 Loss: 0.8689113259315491\n",
            "Epoch: 3 Loss: 0.8289505243301392\n",
            "Epoch: 4 Loss: 0.7923804521560669\n",
            "Epoch: 5 Loss: 0.7688073515892029\n",
            "Epoch: 6 Loss: 0.744735062122345\n",
            "Epoch: 7 Loss: 0.7252783179283142\n",
            "Epoch: 8 Loss: 0.7110788226127625\n",
            "Epoch: 9 Loss: 0.6926876306533813\n",
            "Epoch: 10 Loss: 0.6846606731414795\n",
            "Epoch: 11 Loss: 0.6701433062553406\n",
            "Epoch: 12 Loss: 0.6578955054283142\n",
            "Epoch: 13 Loss: 0.6444637179374695\n",
            "Epoch: 14 Loss: 0.6355376243591309\n",
            "Epoch: 15 Loss: 0.6251114010810852\n",
            "Epoch: 16 Loss: 0.6153188347816467\n",
            "Epoch: 17 Loss: 0.6113913655281067\n",
            "Epoch: 18 Loss: 0.5987008213996887\n",
            "Epoch: 19 Loss: 0.6001372337341309\n",
            "hits@1: 0.777853, hits@5: 0.912961, hits@10: 0.942475, hits@20: 0.963061\n",
            "hits@50: 0.987118, hits@100: 0.996032\n",
            "map@1: 0.777853, map@5: 0.834172, map@10: 0.838151, map@20: 0.839562\n",
            "map@50: 0.840351, map@100: 0.840481\n",
            "ndcg@1: 0.777853, ndcg@5: 0.854113, ndcg@10: 0.863697, ndcg@20: 0.868878\n",
            "ndcg@50: 0.873691, ndcg@100: 0.875144\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "model  = RecModel(num_classes)\n",
        "\n",
        "model.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.97, last_epoch=-1)\n",
        "\n",
        "\n",
        "for i in range(20):\n",
        "  losses = []\n",
        "  hits_1_scores = []\n",
        "\n",
        "  for user, train_input, train_time, train_label, train_time_label, test_input, test_time, test_label, test_time_label, seq_len in user_train_dataloader:\n",
        "      optimizer.zero_grad()\n",
        "      comb_input = np.concatenate([np.expand_dims(train_input, axis=-1),\n",
        "                                                np.expand_dims(train_time, axis=-1)], axis=2)\n",
        "      model_input = comb_input\n",
        "      model_output = train_label\n",
        "      rnn_input_emb = item_emb[model_input[:, :, 0]]\n",
        "\n",
        "\n",
        "      probs = model(rnn_input_emb, seq_len)\n",
        "      pred = torch.argmax(probs, axis = 1)\n",
        "\n",
        "      one_hot = torch.zeros(len(probs), num_classes)\n",
        "      one_hot[torch.arange(len(one_hot)), model_output] = 1\n",
        "\n",
        "      loss = loss_fn(probs, one_hot)\n",
        "      losses.append(loss)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      losses.append(loss)\n",
        "\n",
        "\n",
        "  mean_loss = torch.Tensor(losses).mean(axis=0)\n",
        "  mean_hits = torch.Tensor(hits_1_scores).mean(axis=0).item()\n",
        "  print(f'Epoch: {i} Loss: {mean_loss.item()}')\n",
        "hits1, hits5, hits10, hits20, hits50, hits100, map1, map5, map10, map20, map50, map100, ndcg1, ndcg5, ndcg10, ndcg20, ndcg50, ndcg100 = test(model)\n",
        "print_metrics(hits1, hits5, hits10, hits20, hits50, hits100, map1, map5, map10, map20, map50, map100, ndcg1, ndcg5, ndcg10, ndcg20, ndcg50, ndcg100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6DddS_OUCAs",
        "outputId": "d68aef5e-7837-4733-ab27-720ff0363710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hits@1: 0.777387, hits@5: 0.912961, hits@10: 0.942475, hits@20: 0.963061\n",
            "hits@50: 0.987118, hits@100: 0.995799\n",
            "map@1: 0.777387, map@5: 0.833931, map@10: 0.837911, map@20: 0.839321\n",
            "map@50: 0.840110, map@100: 0.840236\n",
            "ndcg@1: 0.777387, ndcg@5: 0.853937, ndcg@10: 0.863520, ndcg@20: 0.868702\n",
            "ndcg@50: 0.873515, ndcg@100: 0.874928\n"
          ]
        }
      ],
      "source": [
        "hits1, hits5, hits10, hits20, hits50, hits100, map1, map5, map10, map20, map50, map100, ndcg1, ndcg5, ndcg10, ndcg20, ndcg50, ndcg100 = test(model)\n",
        "print_metrics(hits1, hits5, hits10, hits20, hits50, hits100, map1, map5, map10, map20, map50, map100, ndcg1, ndcg5, ndcg10, ndcg20, ndcg50, ndcg100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'rnn_baseline_model.pth')"
      ],
      "metadata": {
        "id": "wXEf7Nlw9860"
      },
      "execution_count": 34,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}