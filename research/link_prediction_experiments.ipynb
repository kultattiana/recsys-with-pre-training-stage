{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "My_CXpFlxUvA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import random\n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import top_k_accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ox0nfVzoKySr"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6o_NKJOFI09u"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WiHOR41FI09u"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Util function"
      ],
      "metadata": {
        "id": "aZ1KY0fsx_Nm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "e4mUB-lFrI5U"
      },
      "outputs": [],
      "source": [
        "def load_dataset_timestamp( n_users, n_context, seq_len):\n",
        "    act_list = []\n",
        "    time_list = []\n",
        "    user_list = []\n",
        "\n",
        "    max_timestamp = -1.0\n",
        "    min_timestamp = float('inf')\n",
        "\n",
        "    with open('gowalla_user_activity.txt', 'r') as raw_file:\n",
        "        for line in raw_file:\n",
        "            t_item_list = []\n",
        "            t_time_list = []\n",
        "            user = int(line.split(':')[0])\n",
        "            entries = line.split()[1:]\n",
        "            for a_entry in entries:\n",
        "                item, time_stamp = a_entry.split(':')\n",
        "                t_item_list.append(int(item.strip()))\n",
        "                t_time_list.append(int(time_stamp.strip()))\n",
        "\n",
        "                if min_timestamp > int(time_stamp.strip()):\n",
        "                    min_timestamp = int(time_stamp.strip())\n",
        "                if max_timestamp < int(time_stamp.strip()):\n",
        "                    max_timestamp = int(time_stamp.strip())\n",
        "\n",
        "            act_list.append(t_item_list[0: seq_len])\n",
        "            time_list.append(t_time_list[0: seq_len])\n",
        "            user_list.append(user)\n",
        "\n",
        "    new_time_list = []\n",
        "    num_bins = 0\n",
        "\n",
        "    times_bins = np.linspace(min_timestamp, max_timestamp + 1, num=num_bins, dtype=np.int32)\n",
        "    for a_time_list in time_list:\n",
        "        temp_time_list = (np.digitize(np.asarray(a_time_list), times_bins) - 1).tolist()\n",
        "        new_time_list.append(temp_time_list)\n",
        "\n",
        "    all_examples = []\n",
        "    for i in range(0, len(act_list)):\n",
        "\n",
        "        train_act_seq = act_list[i][:-2]\n",
        "\n",
        "        train_time_seq = new_time_list[i][:-2]\n",
        "\n",
        "        train_act_label = act_list[i][-2]\n",
        "        train_time_label = new_time_list[i][-2]\n",
        "\n",
        "        test_act_seq = act_list[i][1:-1]\n",
        "        test_time_seq = new_time_list[i][1:-1]\n",
        "\n",
        "        test_act_label = act_list[i][-1]\n",
        "        test_time_label = new_time_list[i][-1]\n",
        "\n",
        "        entry = {\n",
        "            'train_act_seq': train_act_seq,\n",
        "            'train_time_seq': train_time_seq,\n",
        "            'train_act_label': train_act_label,\n",
        "            'train_time_label': train_time_label,\n",
        "            'test_act_seq': test_act_seq,\n",
        "            'test_time_seq': test_time_seq,\n",
        "            'test_act_label': test_act_label,\n",
        "            'test_time_label': test_time_label,\n",
        "            'seq_len': len(train_act_seq),\n",
        "            'user': user_list[i]\n",
        "        }\n",
        "\n",
        "        all_examples.append(entry)\n",
        "\n",
        "    return all_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "mDM_Z8mjKWoe"
      },
      "outputs": [],
      "source": [
        "data_examples = load_dataset_timestamp(20001, 128, 30)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter(x):\n",
        "  return [user for user in users if user not in x]"
      ],
      "metadata": {
        "id": "YudVMZuAJLwx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics"
      ],
      "metadata": {
        "id": "zu-By-7F2tdy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "FmEaZ3v-PPB2"
      },
      "outputs": [],
      "source": [
        "def print_metrics(hits1, hits5, hits10, hits20, hits50, hits100, map1, map5, map10, map20, map50, map100, ndcg1, ndcg5, ndcg10, ndcg20, ndcg50, ndcg100):\n",
        "    print(f'hits@1: {hits1:.6f}, hits@5: {hits5:.6f}, hits@10: {hits10:.6f}, hits@20: {hits20:.6f}')\n",
        "    print(f'hits@50: {hits50:.6f}, hits@100: {hits100:.6f}')\n",
        "    print(f'map@1: {map1:.6f}, map@5: {map5:.6f}, map@10: {map10:.6f}, map@20: {map20:.6f}')\n",
        "    print(f'map@50: {map50:.6f}, map@100: {map100:.6f}')\n",
        "    print(f'ndcg@1: {ndcg1:.6f}, ndcg@5: {ndcg5:.6f}, ndcg@10: {ndcg10:.6f}, ndcg@20: {ndcg20:.6f}')\n",
        "    print(f'ndcg@50: {ndcg50:.6f}, ndcg@100: {ndcg100:.6f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "wveiHrarPAc2"
      },
      "outputs": [],
      "source": [
        "def apk(actual, predicted, k=10):\n",
        "\n",
        "    if len(predicted) > k:\n",
        "        predicted = predicted[:k]\n",
        "\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "\n",
        "    for i, p in enumerate(predicted):\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i + 1.0)\n",
        "\n",
        "    if not actual:\n",
        "        return 0.0\n",
        "\n",
        "    return score / min(len(actual), k)\n",
        "\n",
        "\n",
        "def mapk(y_prob, y, k=10):\n",
        "\n",
        "    predicted = [np.argsort(p_)[-k:][::-1] for p_ in y_prob]\n",
        "    actual = [[y_] for y_ in y]\n",
        "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
        "\n",
        "\n",
        "def hits_k(y_prob, y, k=10):\n",
        "    acc = []\n",
        "    for p_, y_ in zip(y_prob, y):\n",
        "        top_k = p_.argsort()[-k:][::-1]\n",
        "        acc += [1. if y_ in top_k else 0.]\n",
        "from sklearn.metrics import ndcg_score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics_(probs, labels_batch, test_one_hot):\n",
        "    hits1 = top_k_accuracy_score(labels_batch, probs.cpu().detach().numpy(), k=1, labels = classes)\n",
        "    hits5 = top_k_accuracy_score(labels_batch, probs.cpu().detach().numpy(), k=5, labels = classes)\n",
        "    hits10 = top_k_accuracy_score(labels_batch, probs.cpu().detach().numpy(), k=10, labels = classes)\n",
        "    hits20 = top_k_accuracy_score(labels_batch, probs.cpu().detach().numpy(), k=20, labels = classes)\n",
        "    hits50= top_k_accuracy_score(labels_batch, probs.cpu().detach().numpy(), k=50, labels = classes)\n",
        "    hits100 = top_k_accuracy_score(labels_batch, probs.cpu().detach().numpy(), k=100, labels = classes)\n",
        "\n",
        "    map1 = mapk(y_prob=probs.cpu().detach().numpy(), y = labels_batch, k=1)\n",
        "    map5 = mapk(y_prob=probs.cpu().detach().numpy(), y = labels_batch, k=5)\n",
        "    map10 = mapk(y_prob=probs.cpu().detach().numpy(), y = labels_batch, k=10)\n",
        "    map20 = mapk(y_prob=probs.cpu().detach().numpy(), y = labels_batch, k=20)\n",
        "    map50 = mapk(y_prob=probs.cpu().detach().numpy(), y = labels_batch, k=50)\n",
        "    map100 = mapk(y_prob=probs.cpu().detach().numpy(), y = labels_batch, k=100)\n",
        "\n",
        "    ndcg1 = ndcg_score(test_one_hot, probs.cpu().detach().numpy(), k=1)\n",
        "    ndcg5 = ndcg_score(test_one_hot, probs.cpu().detach().numpy(), k=5)\n",
        "    ndcg10 = ndcg_score(test_one_hot, probs.cpu().detach().numpy(), k=10)\n",
        "    ndcg20 = ndcg_score(test_one_hot, probs.cpu().detach().numpy(), k=20)\n",
        "    ndcg50 = ndcg_score(test_one_hot, probs.cpu().detach().numpy(), k=50)\n",
        "    ndcg100 = ndcg_score(test_one_hot, probs.cpu().detach().numpy(), k=100)\n",
        "    return hits1, hits5, hits10, hits20, hits50, hits100, map1, map5, map10, map20, map50, map100, ndcg1, ndcg5, ndcg10, ndcg20, ndcg50, ndcg100\n",
        "    return sum(acc) / len(acc)"
      ],
      "metadata": {
        "id": "9rYVlp7z3B7S"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pairs construction"
      ],
      "metadata": {
        "id": "e6pB1mHWtpmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "friends = pd.read_csv('gowalla_edges.csv')\n",
        "relationships = friends.groupby('1st friend').agg({'2nd friend': lambda x: list(x)}).rename(columns={'2nd friend':'friends'})\n",
        "users = [x for x in range(0, 20001)]\n",
        "relationships['not friends'] = relationships['friends'].map(lambda x: filter(x))\n",
        "relationships = relationships.reset_index()\n",
        "relationships.apply(lambda x: x['not friends'].remove(x['1st friend']), axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m7AHq-XJBrM",
        "outputId": "35f90cea-67de-49f8-edfa-d6f39c2373c7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        None\n",
              "1        None\n",
              "2        None\n",
              "3        None\n",
              "4        None\n",
              "         ... \n",
              "19721    None\n",
              "19722    None\n",
              "19723    None\n",
              "19724    None\n",
              "19725    None\n",
              "Length: 19726, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = []\n",
        "num_users = len(users)\n",
        "for i in range(relationships.shape[0]):\n",
        "  user = relationships.loc[i, :]\n",
        "  if len(user['friends']) >= 1:\n",
        "    random_friends = random.sample(user['friends'], 1)\n",
        "    random_not_friends = random.sample(user['not friends'], 1)\n",
        "  else:\n",
        "    random_friends = user['friends']\n",
        "    random_not_friends = random.sample(user['not friends'], 1)\n",
        "  for friend in random_friends:\n",
        "    pairs.append((user['1st friend'], friend, 1))\n",
        "  for noname in random_not_friends:\n",
        "    pairs.append((user['1st friend'], noname, 0))\n",
        "\n",
        "for i in range(0, num_users):\n",
        "  if i not in relationships['1st friend'].tolist():\n",
        "    not_friends = [x for x in range(0, num_users)]\n",
        "    not_friends.remove(i)\n",
        "    random_not_friends = random.sample(not_friends, 1)\n",
        "  for noname in random_not_friends:\n",
        "    pairs.append((i, noname, 0))"
      ],
      "metadata": {
        "id": "Ej0YkXKSJYvR"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PairDataset():\n",
        "  def __init__(self, pairs, data, max_len):\n",
        "    self.pairs = pairs\n",
        "    self.data = data\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.pairs)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    first_friend = self.pairs[idx][0]\n",
        "    second_friend = self.pairs[idx][1]\n",
        "    link = self.pairs[idx][2]\n",
        "    node_1 = self.data[first_friend]\n",
        "    node_2 = self.data[second_friend]\n",
        "    seq_len_1 = node_1['seq_len']\n",
        "    seq_len_2 = node_2['seq_len']\n",
        "\n",
        "    tr_act_seq_1 = np.zeros((self.max_len,)).astype('int32')\n",
        "    tr_act_seq_1[:seq_len_1] = np.array(node_1['train_act_seq'])\n",
        "    tr_act_seq_1 = np.transpose(tr_act_seq_1)\n",
        "\n",
        "    tr_time_seq_1 = np.zeros((self.max_len,)).astype('int32')\n",
        "    tr_time_seq_1[:seq_len_1] = node_1['train_time_seq']\n",
        "    tr_time_seq_1 = np.transpose(tr_time_seq_1)\n",
        "\n",
        "    t_act_seq_1 = np.zeros((self.max_len, )).astype('int32')\n",
        "    t_act_seq_1[:seq_len_1] = node_1['test_act_seq']\n",
        "    t_act_seq_1 = np.transpose(t_act_seq_1)\n",
        "\n",
        "    t_time_seq_1 = np.zeros((self.max_len, )).astype('int32')\n",
        "    t_time_seq_1[:seq_len_1] = node_1['test_time_seq']\n",
        "    t_time_seq_1 = np.transpose(t_time_seq_1)\n",
        "\n",
        "    tr_act_seq_2 = np.zeros((self.max_len, )).astype('int32')\n",
        "    tr_act_seq_2[:seq_len_2] = node_2['train_act_seq']\n",
        "    tr_act_seq_2 = np.transpose(tr_act_seq_2)\n",
        "\n",
        "    tr_time_seq_2 = np.zeros((self.max_len,)).astype('int32')\n",
        "    tr_time_seq_2[:seq_len_2] = node_2['train_time_seq']\n",
        "    tr_time_seq_2 = np.transpose(tr_time_seq_2)\n",
        "\n",
        "    t_act_seq_2 = np.zeros((self.max_len,)).astype('int32')\n",
        "    t_act_seq_2[:seq_len_2] = node_2['test_act_seq']\n",
        "    t_act_seq_2 = np.transpose(t_act_seq_2)\n",
        "\n",
        "    t_time_seq_2 = np.zeros((self.max_len,)).astype('int32')\n",
        "    t_time_seq_2[:seq_len_2] = node_2['test_time_seq']\n",
        "    t_time_seq_2 = np.transpose(t_time_seq_2)\n",
        "\n",
        "    return node_1['user'], tr_act_seq_1, \\\n",
        "    tr_time_seq_1, node_1['train_act_label'], \\\n",
        "    node_1['train_time_label'], t_act_seq_1, \\\n",
        "    t_time_seq_1, node_1['test_act_label'], \\\n",
        "    node_1['test_time_label'], node_1['seq_len'], \\\n",
        "    node_2['user'], tr_act_seq_2, \\\n",
        "    tr_time_seq_2, node_2['train_act_label'], \\\n",
        "    node_2['train_time_label'], t_act_seq_2, \\\n",
        "    t_time_seq_2, node_2['test_act_label'], \\\n",
        "    node_2['test_time_label'], node_2['seq_len'], link"
      ],
      "metadata": {
        "id": "nzTqhRLOJsET"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pair_dataset = PairDataset(pairs, data_examples, 30)\n",
        "user_1, train_input_1, train_time_1, train_label_1, train_time_label_1, test_input_1, test_time_1, test_label_1, test_time_label_1, seq_len_1, \\\n",
        "user_2, train_input_2, train_time_2, train_label_2, train_time_label_2, test_input_2, test_time_2, test_label_2, test_time_label_2, seq_len_2, \\\n",
        "link = pair_dataset[0]"
      ],
      "metadata": {
        "id": "TSoK8eMOKOrO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UserDataset():\n",
        "    def __init__(self, user_representations):\n",
        "        self.user_representations = user_representations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.user_representations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        user_id = idx\n",
        "        if idx not in self.user_representations.keys():\n",
        "          return user_id, self.user_representations[1][0], 0\n",
        "        return user_id, self.user_representations[idx][0], self.user_representations[idx][1]"
      ],
      "metadata": {
        "id": "HUI0f58FKSTh"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 186\n",
        "classes = np.arange(0, num_classes)"
      ],
      "metadata": {
        "id": "YRdEhQcAuHCj"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target prediction + Link prediction model"
      ],
      "metadata": {
        "id": "Rlw0_SDDxp1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TargetWithLinkPredictionModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(TargetWithLinkPredictionModel, self).__init__()\n",
        "\n",
        "    self.rnn = nn.RNN(128, 128, batch_first = True)\n",
        "    self.dropout = nn.Dropout(p=0.3)\n",
        "    self.relu = nn.Tanh()\n",
        "    self.norm = nn.BatchNorm1d(128)\n",
        "    self.head_1 = nn.Sequential(nn.Linear(256, 2))\n",
        "    self.head_2 = nn.Linear(128, num_classes)\n",
        "    self.sigmoid= nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x_1, x_2, seq_len_1, seq_len_2):\n",
        "\n",
        "    x_1, h_1 = self.rnn(x_1)\n",
        "    hx_1 = torch.zeros(x_1.shape[0], x_1.shape[2])\n",
        "    for i in range(hx_1.shape[0]):\n",
        "      hx_1[i] = x_1[i][seq_len_1[i] - 1]\n",
        "    hx_1 = self.norm(hx_1)\n",
        "\n",
        "    x_2, h_2 = self.rnn(x_2)\n",
        "    hx_2 = torch.zeros(x_2.shape[0], x_2.shape[2])\n",
        "    for i in range(hx_2.shape[0]):\n",
        "      hx_2[i] = x_2[i][seq_len_2[i] - 1]\n",
        "    hx_2 = self.norm(hx_2)\n",
        "\n",
        "    x = torch.cat((hx_1, hx_2), dim = 1)\n",
        "    link_pred = self.head_1(x)\n",
        "    next_pred = self.head_2(hx_1)\n",
        "    return link_pred, next_pred, hx_1, hx_2"
      ],
      "metadata": {
        "id": "jRCiuWNdKb-6"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset construction"
      ],
      "metadata": {
        "id": "eqqpRxijx4PW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pair_dataset = PairDataset(pairs, data_examples, 30)"
      ],
      "metadata": {
        "id": "6rwaha6OvFiR"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = len(pair_dataset)\n",
        "\n",
        "indices = np.arange(n)\n",
        "indices = np.random.permutation(indices)\n",
        "\n",
        "train_indices = indices [:int(0.8*n)]\n",
        "test_indices = indices[int(0.8*n):]\n",
        "\n",
        "pair_train_dataset = Subset(pair_dataset, train_indices)\n",
        "pair_test_dataset = Subset(pair_dataset, test_indices)\n",
        "\n",
        "pair_train_dataloader = DataLoader(pair_train_dataset, batch_size=64, shuffle=True)\n",
        "pair_test_dataloader = DataLoader(pair_test_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "l99x8VHUwVA5"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_emb  = nn.init.xavier_uniform_(torch.empty(num_classes, 128))"
      ],
      "metadata": {
        "id": "BNkXXTfLwZsw"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target prediction + Link prediction training"
      ],
      "metadata": {
        "id": "C6NmW_p5yR2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "link_loss_fn = nn.CrossEntropyLoss()\n",
        "next_loss_fn = nn.CrossEntropyLoss()\n",
        "model  = TargetWithLinkPredictionModel()\n",
        "\n",
        "train_user_representations = {}\n",
        "test_user_representations = {}\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.97, last_epoch=-1)\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "for i in range(epochs):\n",
        "  metrics_val = []\n",
        "  hits_1_scores = []\n",
        "  losses = []\n",
        "\n",
        "  for user_1, train_input_1, train_time_1, train_label_1, train_time_label_1, test_input_1, test_time_1, test_label_1, test_time_label_1, seq_len_1, \\\n",
        "      user_2, train_input_2, train_time_2, train_label_2, train_time_label_2, test_input_2, test_time_2, test_label_2, test_time_label_2, seq_len_2, \\\n",
        "      link in pair_train_dataloader:\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "\n",
        "      comb_input_1 = np.concatenate([np.expand_dims(train_input_1, axis=-1),\n",
        "                                                np.expand_dims(train_time_1, axis=-1)], axis=2)\n",
        "      comb_input_2 = np.concatenate([np.expand_dims(train_input_2, axis=-1),\n",
        "                                                np.expand_dims(train_time_2, axis=-1)], axis=2)\n",
        "      model_input_1 = comb_input_1\n",
        "      model_input_2 = comb_input_2\n",
        "\n",
        "      model_output = link\n",
        "      rnn_input_emb_1 = item_emb[model_input_1[:, :, 0]]\n",
        "      rnn_input_emb_2 = item_emb[model_input_2[:, :, 0]]\n",
        "\n",
        "      seq_len_1 = torch.Tensor(seq_len_1).to(torch.int32)\n",
        "      seq_len_2 = torch.Tensor(seq_len_2).to(torch.int32)\n",
        "\n",
        "\n",
        "      link_prob, next_prob, hx_1, hx_2 = model(rnn_input_emb_1, rnn_input_emb_2, seq_len_1, seq_len_2)\n",
        "\n",
        "      pred = torch.argmax(link_prob, axis = 1)\n",
        "      one_hot = torch.zeros(len(link_prob), 2)\n",
        "      one_hot[torch.arange(len(one_hot)), model_output] = 1\n",
        "      model_output = torch.Tensor(model_output).view(1, -1).permute(1, 0)\n",
        "\n",
        "      link_loss = link_loss_fn(link_prob, one_hot)\n",
        "      next_one_hot = torch.zeros(len(next_prob), 186)\n",
        "      next_one_hot[torch.arange(len(next_one_hot)), train_label_1] = 1\n",
        "      next_loss = next_loss_fn(next_prob, next_one_hot)\n",
        "\n",
        "      loss = link_loss + 1.5 * next_loss\n",
        "\n",
        "      losses.append(loss)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      hits1, hits5, hits10, hits20, hits50, hits100, map1, map5, map10, map20, map50, map100,\\\n",
        "      ndcg1, ndcg5, ndcg10, ndcg20, ndcg50, ndcg100 = get_metrics_(next_prob, train_label_1, next_one_hot)\n",
        "      metrics_val.append([hits1, hits5, hits10, hits20, hits50, hits100, map1, map5, map10, map20, map50, map100, ndcg1, ndcg5, ndcg10, ndcg20, ndcg50, ndcg100])\n",
        "\n",
        "  mean = torch.Tensor(metrics_val).mean(axis=0)\n",
        "  mean_loss = torch.Tensor(losses).mean(axis=0).item()\n",
        "  print(f'Epoch: {i}, Loss:{mean_loss}')\n",
        "  print('Training metrics')\n",
        "  hits1, hits5, hits10, hits20, hits50, hits100, map1, map5, map10, map20, map50, map100, \\\n",
        "  ndcg1, ndcg5, ndcg10, ndcg20, ndcg50, ndcg100 = mean\n",
        "  print_metrics(hits1, hits5, hits10, hits20, hits50, hits100, map1, map5, map10, map20, map50, map100, ndcg1, ndcg5, ndcg10, ndcg20, ndcg50, ndcg100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QMTjfWyxi_s",
        "outputId": "4632dc5f-c751-4a90-e113-7cc537a79103"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss:2.056633472442627\n",
            "Training metrics\n",
            "hits@1: 0.811559, hits@5: 0.924878, hits@10: 0.941931, hits@20: 0.956254\n",
            "hits@50: 0.973265, hits@100: 0.985341\n",
            "map@1: 0.811559, map@5: 0.859818, map@10: 0.862153, map@20: 0.863147\n",
            "map@50: 0.863710, map@100: 0.863886\n",
            "ndcg@1: 0.811559, ndcg@5: 0.876356, ndcg@10: 0.881929, ndcg@20: 0.885551\n",
            "ndcg@50: 0.888962, ndcg@100: 0.890930\n",
            "Epoch: 1, Loss:1.566477656364441\n",
            "Training metrics\n",
            "hits@1: 0.836719, hits@5: 0.961084, hits@10: 0.978537, hits@20: 0.989814\n",
            "hits@50: 0.996745, hits@100: 0.999307\n",
            "map@1: 0.836719, map@5: 0.889194, map@10: 0.891583, map@20: 0.892390\n",
            "map@50: 0.892625, map@100: 0.892662\n",
            "ndcg@1: 0.836719, ndcg@5: 0.907439, ndcg@10: 0.913143, ndcg@20: 0.916024\n",
            "ndcg@50: 0.917425, ndcg@100: 0.917842\n",
            "Epoch: 2, Loss:1.4760764837265015\n",
            "Training metrics\n",
            "hits@1: 0.841003, hits@5: 0.966684, hits@10: 0.984606, hits@20: 0.993679\n",
            "hits@50: 0.999013, hits@100: 0.999832\n",
            "map@1: 0.841003, map@5: 0.894248, map@10: 0.896726, map@20: 0.897385\n",
            "map@50: 0.897569, map@100: 0.897582\n",
            "ndcg@1: 0.841003, ndcg@5: 0.912647, ndcg@10: 0.918529, ndcg@20: 0.920860\n",
            "ndcg@50: 0.921944, ndcg@100: 0.922079\n",
            "Epoch: 3, Loss:1.4182385206222534\n",
            "Training metrics\n",
            "hits@1: 0.844636, hits@5: 0.971262, hits@10: 0.986937, hits@20: 0.996052\n",
            "hits@50: 0.999538, hits@100: 0.999937\n",
            "map@1: 0.844636, map@5: 0.898139, map@10: 0.900328, map@20: 0.900982\n",
            "map@50: 0.901103, map@100: 0.901110\n",
            "ndcg@1: 0.844636, ndcg@5: 0.916706, ndcg@10: 0.921872, ndcg@20: 0.924202\n",
            "ndcg@50: 0.924913, ndcg@100: 0.924979\n",
            "Epoch: 4, Loss:1.3754926919937134\n",
            "Training metrics\n",
            "hits@1: 0.848652, hits@5: 0.973832, hits@10: 0.989436, hits@20: 0.996640\n",
            "hits@50: 0.999727, hits@100: 0.999979\n",
            "map@1: 0.848652, map@5: 0.902006, map@10: 0.904174, map@20: 0.904694\n",
            "map@50: 0.904804, map@100: 0.904808\n",
            "ndcg@1: 0.848652, ndcg@5: 0.920274, ndcg@10: 0.925405, ndcg@20: 0.927251\n",
            "ndcg@50: 0.927884, ndcg@100: 0.927926\n",
            "Epoch: 5, Loss:1.3400585651397705\n",
            "Training metrics\n",
            "hits@1: 0.852718, hits@5: 0.975806, hits@10: 0.990066, hits@20: 0.997123\n",
            "hits@50: 0.999853, hits@100: 1.000000\n",
            "map@1: 0.852718, map@5: 0.905305, map@10: 0.907288, map@20: 0.907794\n",
            "map@50: 0.907898, map@100: 0.907900\n",
            "ndcg@1: 0.852718, ndcg@5: 0.923239, ndcg@10: 0.927931, ndcg@20: 0.929736\n",
            "ndcg@50: 0.930306, ndcg@100: 0.930330\n",
            "Epoch: 6, Loss:1.3072901964187622\n",
            "Training metrics\n",
            "hits@1: 0.854562, hits@5: 0.977424, hits@10: 0.991221, hits@20: 0.997291\n",
            "hits@50: 0.999811, hits@100: 1.000000\n",
            "map@1: 0.854562, map@5: 0.907189, map@10: 0.909093, map@20: 0.909538\n",
            "map@50: 0.909630, map@100: 0.909633\n",
            "ndcg@1: 0.854562, ndcg@5: 0.925062, ndcg@10: 0.929587, ndcg@20: 0.931151\n",
            "ndcg@50: 0.931672, ndcg@100: 0.931704\n",
            "Epoch: 7, Loss:1.2802232503890991\n",
            "Training metrics\n",
            "hits@1: 0.857661, hits@5: 0.978948, hits@10: 0.991978, hits@20: 0.997774\n",
            "hits@50: 0.999916, hits@100: 1.000000\n",
            "map@1: 0.857661, map@5: 0.909540, map@10: 0.911358, map@20: 0.911785\n",
            "map@50: 0.911863, map@100: 0.911864\n",
            "ndcg@1: 0.857661, ndcg@5: 0.927204, ndcg@10: 0.931497, ndcg@20: 0.932992\n",
            "ndcg@50: 0.933433, ndcg@100: 0.933448\n",
            "Epoch: 8, Loss:1.2583249807357788\n",
            "Training metrics\n",
            "hits@1: 0.861496, hits@5: 0.979902, hits@10: 0.992860, hits@20: 0.998047\n",
            "hits@50: 0.999916, hits@100: 1.000000\n",
            "map@1: 0.861496, map@5: 0.912383, map@10: 0.914198, map@20: 0.914575\n",
            "map@50: 0.914645, map@100: 0.914646\n",
            "ndcg@1: 0.861496, ndcg@5: 0.929574, ndcg@10: 0.933850, ndcg@20: 0.935182\n",
            "ndcg@50: 0.935570, ndcg@100: 0.935584\n",
            "Epoch: 9, Loss:1.2324095964431763\n",
            "Training metrics\n",
            "hits@1: 0.863642, hits@5: 0.981666, hits@10: 0.993238, hits@20: 0.998131\n",
            "hits@50: 0.999937, hits@100: 0.999979\n",
            "map@1: 0.863642, map@5: 0.914537, map@10: 0.916135, map@20: 0.916497\n",
            "map@50: 0.916565, map@100: 0.916565\n",
            "ndcg@1: 0.863642, ndcg@5: 0.931645, ndcg@10: 0.935441, ndcg@20: 0.936706\n",
            "ndcg@50: 0.937082, ndcg@100: 0.937088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Link prediction model"
      ],
      "metadata": {
        "id": "ZP9OFhuuypG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinkPredictionModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(LinkPredictionModel, self).__init__()\n",
        "\n",
        "    self.rnn = nn.RNN(128, 128, batch_first = True)\n",
        "    self.dropout = nn.Dropout(p=0.3)\n",
        "    self.relu = nn.Tanh()\n",
        "    self.norm = nn.BatchNorm1d(128)\n",
        "    self.head_1 = nn.Sequential(nn.Linear(256, 2))\n",
        "    self.head_2 = nn.Linear(128, num_classes)\n",
        "    self.sigmoid= nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x_1, x_2, seq_len_1, seq_len_2):\n",
        "\n",
        "    x_1, h_1 = self.rnn(x_1)\n",
        "    hx_1 = torch.zeros(x_1.shape[0], x_1.shape[2])\n",
        "    for i in range(hx_1.shape[0]):\n",
        "      hx_1[i] = x_1[i][seq_len_1[i] - 1]\n",
        "    hx_1 = self.norm(hx_1)\n",
        "\n",
        "    x_2, h_2 = self.rnn(x_2)\n",
        "    hx_2 = torch.zeros(x_2.shape[0], x_2.shape[2])\n",
        "    for i in range(hx_2.shape[0]):\n",
        "      hx_2[i] = x_2[i][seq_len_2[i] - 1]\n",
        "    hx_2 = self.norm(hx_2)\n",
        "\n",
        "    x = torch.cat((hx_1, hx_2), dim = 1)\n",
        "    link_pred = self.head_1(x)\n",
        "    return link_pred, hx_1, hx_2"
      ],
      "metadata": {
        "id": "BRU57NUwyaaW"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teaining link prediction model"
      ],
      "metadata": {
        "id": "nF1CHYiX0xK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "link_loss_fn = nn.CrossEntropyLoss()\n",
        "next_loss_fn = nn.CrossEntropyLoss()\n",
        "model  = LinkPredictionModel()\n",
        "\n",
        "train_user_representations = {}\n",
        "test_user_representations = {}\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.97, last_epoch=-1)\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "for i in range(epochs):\n",
        "  metrics_val = []\n",
        "  accuracy_scores = []\n",
        "  losses = []\n",
        "\n",
        "  for user_1, train_input_1, train_time_1, train_label_1, train_time_label_1, test_input_1, test_time_1, test_label_1, test_time_label_1, seq_len_1, \\\n",
        "      user_2, train_input_2, train_time_2, train_label_2, train_time_label_2, test_input_2, test_time_2, test_label_2, test_time_label_2, seq_len_2, \\\n",
        "      link in pair_train_dataloader:\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "\n",
        "      comb_input_1 = np.concatenate([np.expand_dims(train_input_1, axis=-1),\n",
        "                                                np.expand_dims(train_time_1, axis=-1)], axis=2)\n",
        "      comb_input_2 = np.concatenate([np.expand_dims(train_input_2, axis=-1),\n",
        "                                                np.expand_dims(train_time_2, axis=-1)], axis=2)\n",
        "      model_input_1 = comb_input_1\n",
        "      model_input_2 = comb_input_2\n",
        "\n",
        "      model_output = link\n",
        "      rnn_input_emb_1 = item_emb[model_input_1[:, :, 0]]\n",
        "      rnn_input_emb_2 = item_emb[model_input_2[:, :, 0]]\n",
        "\n",
        "      seq_len_1 = torch.Tensor(seq_len_1).to(torch.int32)\n",
        "      seq_len_2 = torch.Tensor(seq_len_2).to(torch.int32)\n",
        "\n",
        "\n",
        "      link_prob, hx_1, hx_2 = model(rnn_input_emb_1, rnn_input_emb_2, seq_len_1, seq_len_2)\n",
        "\n",
        "      pred = torch.argmax(link_prob, axis = 1)\n",
        "      one_hot = torch.zeros(len(link_prob), 2)\n",
        "      one_hot[torch.arange(len(one_hot)), model_output] = 1\n",
        "      model_output = torch.Tensor(model_output).view(1, -1).permute(1, 0)\n",
        "\n",
        "      link_loss = link_loss_fn(link_prob, one_hot)\n",
        "\n",
        "      loss = link_loss\n",
        "      accuracy_scores.append(accuracy_score(model_output, pred))\n",
        "\n",
        "      losses.append(loss)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  mean = torch.Tensor(metrics_val).mean(axis=0)\n",
        "  mean_loss = torch.Tensor(losses).mean(axis=0).item()\n",
        "  mean_accuracy = torch.Tensor(accuracy_scores).mean(axis=0).item()\n",
        "  print(f'Epoch: {i}, Loss:{mean_loss} Train accuracy: {mean_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Lkisn5hzTk1",
        "outputId": "b6c8d85e-b434-427d-f5e1-3c74769c14ac"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss:0.49598291516304016 Train accuracy: 0.7258568406105042\n",
            "Epoch: 1, Loss:0.47170543670654297 Train accuracy: 0.7419102787971497\n",
            "Epoch: 2, Loss:0.4813894033432007 Train accuracy: 0.7369371652603149\n",
            "Epoch: 3, Loss:0.4596621096134186 Train accuracy: 0.74403977394104\n",
            "Epoch: 4, Loss:0.44971615076065063 Train accuracy: 0.7495001554489136\n",
            "Epoch: 5, Loss:0.44246309995651245 Train accuracy: 0.7533811926841736\n",
            "Epoch: 6, Loss:0.43718650937080383 Train accuracy: 0.7574806809425354\n",
            "Epoch: 7, Loss:0.4316729009151459 Train accuracy: 0.7580267190933228\n",
            "Epoch: 8, Loss:0.42781516909599304 Train accuracy: 0.7629326581954956\n",
            "Epoch: 9, Loss:0.4230033755302429 Train accuracy: 0.766427218914032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtaining representations of users from training part"
      ],
      "metadata": {
        "id": "qXZHygu11CmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_user_representations = {}\n",
        "test_user_representations = {}\n",
        "\n",
        "for user_1, train_input_1, train_time_1, train_label_1, train_time_label_1, test_input_1, test_time_1, test_label_1, test_time_label_1, seq_len_1, \\\n",
        "      user_2, train_input_2, train_time_2, train_label_2, train_time_label_2, test_input_2, test_time_2, test_label_2, test_time_label_2, seq_len_2, \\\n",
        "      link in pair_train_dataloader:\n",
        "\n",
        "  comb_input_1 = np.concatenate([np.expand_dims(train_input_1, axis=-1),\n",
        "                                                  np.expand_dims(train_time_1, axis=-1)], axis=2)\n",
        "  comb_input_2 = np.concatenate([np.expand_dims(train_input_2, axis=-1),\n",
        "                                            np.expand_dims(train_time_2, axis=-1)], axis=2)\n",
        "\n",
        "  test_comb_input_1 = np.concatenate([np.expand_dims(test_input_1, axis=-1),\n",
        "                                                  np.expand_dims(test_time_1, axis=-1)], axis=2)\n",
        "  test_comb_input_2 = np.concatenate([np.expand_dims(test_input_2, axis=-1),\n",
        "                                            np.expand_dims(test_time_2, axis=-1)], axis=2)\n",
        "\n",
        "  model_input_1 = comb_input_1\n",
        "  model_input_2 = comb_input_2\n",
        "\n",
        "  test_model_input_1 = test_comb_input_1\n",
        "  test_model_input_2 = test_comb_input_2\n",
        "\n",
        "  model_output = link\n",
        "  rnn_input_emb_1 = item_emb[model_input_1[:, :, 0]]\n",
        "  rnn_input_emb_2 = item_emb[model_input_2[:, :, 0]]\n",
        "\n",
        "  test_rnn_input_emb_1 = item_emb[test_model_input_1[:, :, 0]]\n",
        "  test_rnn_input_emb_2 = item_emb[test_model_input_2[:, :, 0]]\n",
        "\n",
        "  seq_len_1 = torch.Tensor(seq_len_1).to(torch.int32)\n",
        "\n",
        "  seq_len_2 = torch.Tensor(seq_len_2).to(torch.int32)\n",
        "\n",
        "  prob, x_1, x_2 = model(rnn_input_emb_1, rnn_input_emb_2, seq_len_1, seq_len_2)\n",
        "  test_prob, t_x_1, t_x_2 = model(test_rnn_input_emb_1, test_rnn_input_emb_2, seq_len_1, seq_len_2)\n",
        "\n",
        "  model_output = torch.Tensor(model_output).view(1, -1).permute(1, 0)\n",
        "\n",
        "  index_1 = 0\n",
        "  for user in user_1:\n",
        "    user = user.item()\n",
        "    if user not in train_user_representations.keys():\n",
        "      train_user_representations[user] = (x_1[index_1].detach(), train_label_1[index_1])\n",
        "    if user not in test_user_representations.keys():\n",
        "      test_user_representations[user] = (t_x_1[index_1].detach(), test_label_1[index_1])\n",
        "    index_1 += 1\n",
        "  index_2 = 0\n",
        "  for user in user_2:\n",
        "    user = user.item()\n",
        "    if user not in train_user_representations.keys():\n",
        "      train_user_representations[user] = (x_2[index_2].detach(), train_label_2[index_2])\n",
        "    if user not in test_user_representations.keys():\n",
        "      test_user_representations[user] = (t_x_2[index_2].detach(), test_label_2[index_2])\n",
        "    index_2 += 1"
      ],
      "metadata": {
        "id": "X_tsKgq_00H9"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtaining representations of users from testing part"
      ],
      "metadata": {
        "id": "nLLcEfnZ1HqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_scores = []\n",
        "hits_scores = []\n",
        "\n",
        "for user_1, train_input_1, train_time_1, train_label_1, train_time_label_1, test_input_1, test_time_1, test_label_1, test_time_label_1, seq_len_1, \\\n",
        "      user_2, train_input_2, train_time_2, train_label_2, train_time_label_2, test_input_2, test_time_2, test_label_2, test_time_label_2, seq_len_2, \\\n",
        "      link in pair_test_dataloader:\n",
        "\n",
        "  comb_input_1 = np.concatenate([np.expand_dims(train_input_1, axis=-1),\n",
        "                                                  np.expand_dims(train_time_1, axis=-1)], axis=2)\n",
        "  comb_input_2 = np.concatenate([np.expand_dims(train_input_2, axis=-1),\n",
        "                                            np.expand_dims(train_time_2, axis=-1)], axis=2)\n",
        "\n",
        "  test_comb_input_1 = np.concatenate([np.expand_dims(test_input_1, axis=-1),\n",
        "                                                  np.expand_dims(test_time_1, axis=-1)], axis=2)\n",
        "  test_comb_input_2 = np.concatenate([np.expand_dims(test_input_2, axis=-1),\n",
        "                                            np.expand_dims(test_time_2, axis=-1)], axis=2)\n",
        "\n",
        "  model_input_1 = comb_input_1\n",
        "  model_input_2 = comb_input_2\n",
        "\n",
        "  test_model_input_1 = test_comb_input_1\n",
        "  test_model_input_2 = test_comb_input_2\n",
        "\n",
        "  model_output = link\n",
        "\n",
        "  rnn_input_emb_1 = item_emb[model_input_1[:, :, 0]]\n",
        "  rnn_input_emb_2 = item_emb[model_input_2[:, :, 0]]\n",
        "\n",
        "  test_rnn_input_emb_1 = item_emb[test_model_input_1[:, :, 0]]\n",
        "  test_rnn_input_emb_2 = item_emb[test_model_input_2[:, :, 0]]\n",
        "\n",
        "  seq_len_1 = torch.Tensor(seq_len_1).to(torch.int32)\n",
        "\n",
        "  seq_len_2 = torch.Tensor(seq_len_2).to(torch.int32)\n",
        "\n",
        "  prob, x_1, x_2 = model(rnn_input_emb_1, rnn_input_emb_2, seq_len_1, seq_len_2)\n",
        "  test_prob, t_x_1, t_x_2 = model(test_rnn_input_emb_1, test_rnn_input_emb_2, seq_len_1, seq_len_2)\n",
        "\n",
        "  pred = torch.argmax(test_prob, axis = 1)\n",
        "  model_output = torch.Tensor(model_output).view(1, -1).permute(1, 0)\n",
        "  accuracy_scores.append(accuracy_score(model_output, pred))\n",
        "\n",
        "  index_1 = 0\n",
        "  for user in user_1:\n",
        "    user = user.item()\n",
        "    if user not in train_user_representations.keys():\n",
        "      train_user_representations[user] = (x_1[index_1].detach(), train_label_1[index_1])\n",
        "    if user not in test_user_representations.keys():\n",
        "      test_user_representations[user] = (t_x_1[index_1].detach(), test_label_1[index_1])\n",
        "    index_1 += 1\n",
        "\n",
        "  index_2 = 0\n",
        "  for user in user_2:\n",
        "    user = user.item()\n",
        "    if user not in train_user_representations.keys():\n",
        "      train_user_representations[user] = (x_2[index_2].detach(), train_label_2[index_2])\n",
        "    if user not in test_user_representations.keys():\n",
        "      test_user_representations[user] = (t_x_2[index_2].detach(), test_label_2[index_2])\n",
        "    index_2 += 1\n",
        "\n",
        "mean = torch.Tensor(accuracy_scores).mean(axis=0).item()\n",
        "print(f'Test accuracy: {mean}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYGh8D2n1Nn0",
        "outputId": "b60b9993-1a5c-46e6-b28b-ad25c4cdd3bb"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.6769729852676392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### User representationsa dataset construction"
      ],
      "metadata": {
        "id": "i8PNPVF11lRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_dataset = UserDataset(train_user_representations)\n",
        "\n",
        "n = len(user_dataset)\n",
        "\n",
        "indices = np.arange(n)\n",
        "indices = np.random.permutation(indices)\n",
        "\n",
        "train_indices = indices [:int(0.8*n)]\n",
        "test_indices = indices[int(0.8*n):]\n",
        "\n",
        "user_train_dataset = Subset(user_dataset, train_indices)\n",
        "user_test_dataset = Subset(user_dataset, test_indices)\n",
        "\n",
        "user_train_dataloader = DataLoader(user_train_dataset, batch_size=64, shuffle=True)\n",
        "user_test_dataloader = DataLoader(user_test_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "jg2MPTfz2POD"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target task training"
      ],
      "metadata": {
        "id": "9TBCg25L2eNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TargetModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TargetModel, self).__init__()\n",
        "    self.fc1 = nn.Linear(128, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "z_x1nw1L2UDg"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model):\n",
        "  metrics_val = []\n",
        "  model.eval()\n",
        "\n",
        "  for users, vectors, labels in user_test_dataloader:\n",
        "\n",
        "      test_probs = model(vectors)\n",
        "\n",
        "      test_pred = torch.argmax(test_probs, axis = 1)\n",
        "\n",
        "      test_one_hot = torch.zeros(len(test_probs), num_classes)\n",
        "      test_one_hot[torch.arange(len(test_one_hot)), labels] = 1\n",
        "\n",
        "      hits1, hits5, hits10, hits20, hits50, hits100, map1, map5, map10, map20, map50, map100,\\\n",
        "      ndcg1, ndcg5, ndcg10, ndcg20, ndcg50, ndcg100 = get_metrics_(test_probs, labels, test_one_hot)\n",
        "      metrics_val.append([hits1, hits5, hits10, hits20, hits50, hits100, map1, map5, map10, map20, map50, map100, ndcg1, ndcg5, ndcg10, ndcg20, ndcg50, ndcg100])\n",
        "\n",
        "  mean = torch.Tensor(metrics_val).mean(axis=0)\n",
        "  test_hits1, test_hits5, test_hits10, test_hits20, test_hits50, test_hits100, test_map1, test_map5, test_map10, test_map20, test_map50, test_map100, \\\n",
        "  test_ndcg1, test_ndcg5,test_ndcg10, test_ndcg20, test_ndcg50, test_ndcg100 = mean\n",
        "\n",
        "  return test_hits1, test_hits5, test_hits10, test_hits20, test_hits50, test_hits100, test_map1, test_map5, test_map10, test_map20, test_map50, test_map100,\\\n",
        "  test_ndcg1, test_ndcg5,test_ndcg10, test_ndcg20, test_ndcg50, test_ndcg100"
      ],
      "metadata": {
        "id": "QbOUtRiXH9mz"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "model  = TargetModel()\n",
        "\n",
        "model.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.97, last_epoch=-1)\n",
        "losses = []\n",
        "\n",
        "for i in range(30):\n",
        "  for users, vectors, labels in user_train_dataloader:\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      probs = model(vectors)\n",
        "      pred = torch.argmax(probs, axis = 1)\n",
        "      one_hot = torch.zeros(len(probs), num_classes)\n",
        "      one_hot[torch.arange(len(one_hot)), labels] = 1\n",
        "      loss = loss_fn(probs, one_hot)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "hits1, hits5, hits10, hits20, hits50, hits100, map1, map5, map10, map20, map50, map100, \\\n",
        "ndcg1, ndcg5, ndcg10, ndcg20, ndcg50, ndcg100 = test(model)\n",
        "print_metrics(hits1, hits5, hits10, hits20, hits50, hits100, map1, map5, map10, map20, map50, map100, ndcg1, ndcg5, ndcg10, ndcg20, ndcg50, ndcg100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLxZDlhC2ixn",
        "outputId": "cb5be170-2db9-49d8-f19b-2b195e45167b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hits@1: 0.814852, hits@5: 0.900358, hits@10: 0.918681, hits@20: 0.937530\n",
            "hits@50: 0.967773, hits@100: 0.989087\n",
            "map@1: 0.814852, map@5: 0.851421, map@10: 0.853906, map@20: 0.855193\n",
            "map@50: 0.856123, map@100: 0.856433\n",
            "ndcg@1: 0.814852, ndcg@5: 0.863862, ndcg@10: 0.869826, ndcg@20: 0.874566\n",
            "ndcg@50: 0.880507, ndcg@100: 0.883979\n"
          ]
        }
      ]
    }
  ]
}